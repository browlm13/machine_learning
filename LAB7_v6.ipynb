{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import Dataset\n",
    "#\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "DATA_SETS_PATH = 'data/data_sets'\n",
    "DATA_SET_NAME = 'bbcsport'\n",
    "CATEGORIES = ['athletics', 'cricket', 'football', 'rugby', 'tennis']\n",
    "\n",
    "container_path = os.path.join(DATA_SETS_PATH, DATA_SET_NAME)\n",
    "bunch = load_files(container_path=container_path, description=DATA_SET_NAME, categories=CATEGORIES, decode_error='ignore', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Helper Methods\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_dataset_info_string(dataset):\n",
    "    \"\"\"Ridiculous way to print out dataset info\"\"\"\n",
    "    \n",
    "    # count number of samples/documents in dataset\n",
    "    num_docs = lambda i: list(zip(*np.unique(dataset.target, return_counts=True)))[i][1]\n",
    "    \n",
    "    # ordering of output\n",
    "    display_column_order = ['Target', 'Target Name', 'Documents']\n",
    "    \n",
    "    # uses target as index\n",
    "    column_param_funcs = {\n",
    "        'Target' : lambda i: i,\n",
    "        'Target Name' : lambda i: dataset.target_names[i],\n",
    "        'Documents' : lambda i: num_docs(i)\n",
    "    }\n",
    "    \n",
    "    column_names = list(column_param_funcs.keys())\n",
    "    column_headers_dict = {column_name:column_name for column_name in column_names}\n",
    "    column_values = zip(*[[v(i) for v in column_param_funcs.values()] for i in range(len(dataset.target_names))])\n",
    "\n",
    "    # useful dictionaries \n",
    "    info_dict = [{k:v(i) for k,v in column_param_funcs.items()} for i in range(len(dataset.target_names))]\n",
    "    merged_values_by_column = dict(zip(column_names, column_values))    \n",
    "    \n",
    "    # get maximum length string for each column name in dataset\n",
    "    get_max_str_len = lambda list: max([len(str(i)) for i in list])\n",
    "    max_header_len = {k: max(len(k),get_max_str_len(v)) for k,v in merged_values_by_column.items()}\n",
    "    ordered_max_header_len = [(column_name, max_header_len[column_name]) for column_name in display_column_order] \n",
    "    \n",
    "    # format output\n",
    "    template = '|'.join([\"{%s:%d}\" % (column_name, max_len) for column_name, max_len in ordered_max_header_len])\n",
    "    \n",
    "    # create header\n",
    "    header = template.format(**column_headers_dict)\n",
    "    bar = '-'*(sum([o[1] for o in ordered_max_header_len]) + len(ordered_max_header_len))\n",
    "\n",
    "    # add category info to display string\n",
    "    description = dataset.DESCR\n",
    "    if dataset.DESCR is None:\n",
    "        description = \"None\"\n",
    "    data_set_info_string = 'Dataset Description: \\n' + dataset.DESCR + '\\n' + bar + '\\n' + header + '\\n' + bar + '\\n'\n",
    "    for rec in info_dict: \n",
    "          data_set_info_string += template.format(**rec) + '\\n'\n",
    "    data_set_info_string += bar\n",
    "    \n",
    "    # add total number of documents to string\n",
    "    total_documents = dataset.target.shape[0]\n",
    "    data_set_info_string += \"\\nTotal Documents:\\t\" + str(total_documents)\n",
    "\n",
    "            \n",
    "    return data_set_info_string    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Description: \n",
      "bbcsport\n",
      "-----------------------------\n",
      "Target|Target Name|Documents\n",
      "-----------------------------\n",
      "     0|athletics  |      101\n",
      "     1|cricket    |      124\n",
      "     2|football   |      265\n",
      "     3|rugby      |      147\n",
      "     4|tennis     |      100\n",
      "-----------------------------\n",
      "Total Documents:\t737\n",
      "Vocabulary Size:\t14224\n",
      "\n",
      "Pad and limit inputs to RNN... \n",
      "\n",
      "Maximum Document Length: 1724 (words)\n",
      "Mean Document Length: 346.472184531886 (words)\n",
      "STD Document Lengths: 190.3401998942172 \n",
      "\n",
      "Maximum Document Length Allowed: 500 (words)\n",
      "Chosen Document Length Parameter Effects...\n",
      "\n",
      "Percentage of Documents Within Limit: 82.4966078697422\n",
      "STD's Covered: 0.8065969015133854\n",
      "\n",
      "Encoded Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (737, 500)\n",
      "Shape of Label Tensor: (737, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "\n",
    "#\n",
    "# Preproses Text, Convert Documents to Sequences, and One-Hot Encode Targets\n",
    "#\n",
    "\n",
    "# tokenize, build vocab and generate document sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(bunch.data)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(bunch.data)\n",
    "word_index = tokenizer.word_index\n",
    "VOCAB_SIZE = len(word_index)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(bunch.target)\n",
    "\n",
    "#\n",
    "# display dataset info\n",
    "#\n",
    "\n",
    "print(get_dataset_info_string(bunch))\n",
    "print(\"Vocabulary Size:\\t\" + str(VOCAB_SIZE))\n",
    "\n",
    "#\n",
    "# padding and clipping documents for reccurent network input\n",
    "#\n",
    "\n",
    "print(\"\\nPad and limit inputs to RNN... \\n\")\n",
    "\n",
    "# pad and clip\n",
    "MAX_ART_LEN = 500\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "# display document lengths and stats\n",
    "document_lengths = np.array([len(d) for d in sequences])\n",
    "MAX_DOCUEMNT_LENGTH, mean_length, std_length = max(document_lengths), np.mean(document_lengths), np.std(document_lengths)\n",
    "\n",
    "print(\"Maximum Document Length: %s (words)\" % MAX_DOCUEMNT_LENGTH)\n",
    "print(\"Mean Document Length: %s (words)\" % mean_length)\n",
    "print(\"STD Document Lengths: %s \" % std_length)\n",
    "sns.distplot(list(document_lengths), kde=False, rug=True);\n",
    "\n",
    "# MAX_ART_LEN parameter effects\n",
    "docs_covered = document_lengths[ np.where( document_lengths <= MAX_ART_LEN ) ]\n",
    "percent_docs_covered = len(docs_covered)/len(document_lengths)\n",
    "stds_covered = (MAX_ART_LEN - mean_length)/std_length\n",
    "\n",
    "print(\"\\nMaximum Document Length Allowed: %s (words)\" % MAX_ART_LEN)\n",
    "print(\"Chosen Document Length Parameter Effects...\\n\")\n",
    "print(\"Percentage of Documents Within Limit: %s\" % (percent_docs_covered*100))\n",
    "print(\"STD's Covered: %s\" % stds_covered)\n",
    "\n",
    "# display final specs\n",
    "print(\"\\nEncoded Dataset Dimensions:\\n\")\n",
    "print('Shape of Data Tensor:', X.shape)\n",
    "print('Shape of Label Tensor:', y_ohe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Description: \n",
      "bbcsport\n",
      "-----------------------------\n",
      "Target|Target Name|Documents\n",
      "-----------------------------\n",
      "     0|athletics  |      101\n",
      "     1|cricket    |      124\n",
      "     2|football   |      265\n",
      "     3|rugby      |      147\n",
      "     4|tennis     |      100\n",
      "-----------------------------\n",
      "Total Documents:\t737\n",
      "\n",
      "Encoded Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (737, 500)\n",
      "Shape of Label Tensor: (737, 5)\n",
      "\n",
      "Train Test Split:  0.2\n",
      "\n",
      "Train Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (589, 500)\n",
      "Shape of Label Tensor: (589, 5)\n",
      "\n",
      "Test Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (148, 500)\n",
      "Shape of Label Tensor: (148, 5)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Seperate Train and Test Data\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_2_SET_RATIO = 0.2\n",
    "DOUGLAS_ADAMS = 42\n",
    "NUM_CLASSES = len(bunch.target_names)\n",
    "\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=TEST_2_SET_RATIO,\n",
    "                                                            stratify=bunch.target, \n",
    "                                                            random_state=DOUGLAS_ADAMS)\n",
    "#\n",
    "# display dataset train / test split info\n",
    "#\n",
    "\n",
    "print(get_dataset_info_string(bunch))\n",
    "\n",
    "print(\"\\nEncoded Dataset Dimensions:\\n\")\n",
    "print('Shape of Data Tensor:', X.shape)\n",
    "print('Shape of Label Tensor:', y_ohe.shape)\n",
    "\n",
    "print(\"\\nTrain Test Split: \", TEST_2_SET_RATIO)\n",
    "\n",
    "print(\"\\nTrain Dataset Dimensions:\\n\")\n",
    "print('Shape of Data Tensor:', X_train.shape)\n",
    "print('Shape of Label Tensor:', y_train_ohe.shape)\n",
    "\n",
    "print(\"\\nTest Dataset Dimensions:\\n\")\n",
    "print('Shape of Data Tensor:', X_test.shape)\n",
    "print('Shape of Label Tensor:', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_SETS_PATH = 'data/data_sets'\n",
    "\n",
    "#\n",
    "# Save and load word index\n",
    "#\n",
    "\n",
    "def save_word_index(word_index_file, word_index):\n",
    "    with open(word_index_file, \"w\") as json_file:\n",
    "        #json_file.write(word_index)\n",
    "        json.dump(word_index, json_file)\n",
    "\n",
    "        \n",
    "def load_word_index(word_index_file):\n",
    "    json_file = open(word_index_file, 'r')\n",
    "    json_string = json_file.read()\n",
    "    json_file.close()\n",
    "    #json_string = u'{ \"id\":\"123456789\", ... }'\n",
    "    word_index = json.loads(json_string)  \n",
    "    return word_index \n",
    "    \n",
    "#\n",
    "# Preproses Text, One-Hot Encode Targets, Return Word Index\n",
    "#\n",
    "\n",
    "# format data for rnn input and word index\n",
    "def format_data(bunch, max_article_len):\n",
    "\n",
    "    # tokenize, build vocab and generate document sequences\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(bunch.data)\n",
    "    sequences = tokenizer.texts_to_sequences(bunch.data)\n",
    "\n",
    "    # pad and clip\n",
    "    X = pad_sequences(sequences, maxlen=max_article_len)\n",
    "    y_ohe = keras.utils.to_categorical(bunch.target)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    return X, y_ohe, word_index\n",
    "\n",
    "#\n",
    "# Import Dataset\n",
    "#\n",
    "\n",
    "def load_dataset(dataset_name, categories=None):\n",
    "    \n",
    "    global DATA_SETS_PATH\n",
    "    \n",
    "    container_path = os.path.join(DATA_SETS_PATH, dataset_name)\n",
    "    bunch = load_files(container_path=container_path, description=DATA_SET_NAME, \\\n",
    "                       categories=categories, decode_error='ignore', encoding='utf-8')\n",
    "    \n",
    "    return bunch\n",
    "\n",
    "#\n",
    "# Format RNN input and Seperate Train and Test Data (and return word index)\n",
    "# \n",
    "\n",
    "def ready_input(bunch, max_article_len, word_index_file, test_ratio=0.2, random_seed=42):\n",
    "    \n",
    "    # Format Data\n",
    "    X, y_ohe, word_index = format_data(bunch, max_article_len)\n",
    "    \n",
    "    # save word_index file\n",
    "    save_word_index(word_index_file, word_index)\n",
    "\n",
    "    # Split it into train / test subsets\n",
    "    X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=test_ratio,\n",
    "                                                                stratify=bunch.target, \n",
    "                                                                random_state=random_seed)\n",
    "    return X_train, X_test, y_train_ohe, y_test_ohe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create glove embedding matrix from gloves pretrained models\n",
    "#\n",
    "\n",
    "def format_glove_embedding_matrix(dimensions, word_index):\n",
    "    \"\"\" \n",
    "        returns embedding_matrix corresponding to word_index columns\n",
    "        \n",
    "        embdedding_index \n",
    "            format: {key : word, value : word vector}\n",
    "            \n",
    "        Note: unfound words in word_index will be zero vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # the embedding dimensions should match the file you load glove from\n",
    "    assert dimensions in [50, 100, 200, 300]\n",
    "    \n",
    "    GLOVE_EMBEDDINGS_FILE_TEMPLATE = 'data/glove/glove.6B.%sd.txt'\n",
    "    glove_file = GLOVE_EMBEDDINGS_FILE_TEMPLATE % dimensions\n",
    "    \n",
    "    #\n",
    "    # create embeddings index\n",
    "    #\n",
    "    \n",
    "    # format: {key : word, value : word vector} \n",
    "    embeddings_index = {}\n",
    "    \n",
    "    # load glove embeddings file and fill index\n",
    "    f = open(glove_file)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    #\n",
    "    # build embedding matrix coressponding to given word_index\n",
    "    #    Note: words not found in embedding index will be all-zeros.\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, dimensions))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # return index and matrix\n",
    "    return embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#\n",
    "# helper methods\n",
    "#\n",
    "\n",
    "def score_model(x, y, model, name, batch_size):\n",
    "    score = model.evaluate(x, y,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "    for i,metric in enumerate(model.metrics_names):\n",
    "        print(\"%s %s: %.2f%%\" % (name, metric, score[i]))\n",
    "        \n",
    "def plot_training_history(history):\n",
    "    # [TODO]: make not shitty\n",
    "    #print(history.history.keys())\n",
    "\n",
    "    pyplot.plot(history.history['acc'], label='accuracy')\n",
    "    pyplot.plot(history.history['loss'], label='categorical_crossentropy')\n",
    "\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    \n",
    "#\n",
    "#  Save and load model_parameters\n",
    "#\n",
    "MODEL_PARAM_FILE_TEMPLATE = '%s/%s_parameters.json'\n",
    "\n",
    "def save_model_parameters(model_id, model_parameters, model_dir):\n",
    "    \n",
    "    global MODEL_PARAM_FILE_TEMPLATE\n",
    "    model_params_file = MODEL_PARAM_FILE_TEMPLATE % (model_dir, model_id)\n",
    "    \n",
    "    model_json = {\n",
    "        'model_id' : model_id,\n",
    "        'build_params' : model_parameters\n",
    "    }\n",
    "    \n",
    "    with open(model_params_file, \"w\") as f:\n",
    "        f.write(json.dumps(model_json))\n",
    "\n",
    "        \n",
    "def load_model_parameters(model_id, model_dir):\n",
    "    \n",
    "    global MODEL_PARAM_FILE_TEMPLATE\n",
    "    model_params_file = MODEL_PARAM_FILE_TEMPLATE % (model_dir, model_id)\n",
    "    \n",
    "    json_file = open(model_params_file, 'r')\n",
    "    model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    return model_json    \n",
    "\n",
    "\n",
    "#\n",
    "# Build Simple RNN Model\n",
    "#\n",
    "\n",
    "def build_simple_RNN_model(word_index_file=None, max_article_length=None, n_classes=None, \\\n",
    "                    RNN_type='GRU', embed_size=50, state_size=100, dropout=0.2, \\\n",
    "                    recurrent_dropout=0.2, activation='sigmoid', loss='categorical_crossentropy', \\\n",
    "                    optimizer='rmsprop', metrics=['accuracy'] ):\n",
    "\n",
    "    assert RNN_type in ['LSTM', 'GRU']\n",
    "    \n",
    "    # load word_index file\n",
    "    word_index = load_word_index(word_index_file)\n",
    "\n",
    "    # load pre-trained glove embeddings matrix\n",
    "    embedding_matrix = format_glove_embedding_matrix(embed_size, word_index)\n",
    "    \n",
    "    #\n",
    "    # create rnn\n",
    "    #\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1, embed_size,\n",
    "                                weights=[embedding_matrix], input_length=max_article_length,\n",
    "                                trainable=False)\n",
    "    rnn = Sequential()\n",
    "    rnn.add(embedding_layer)\n",
    "    if RNN_type == 'LSTM':\n",
    "        rnn.add(LSTM(embed_size, dropout=dropout, recurrent_dropout=recurrent_dropout ))\n",
    "    else:\n",
    "        rnn.add(GRU(embed_size, dropout=dropout, recurrent_dropout=recurrent_dropout ))\n",
    "    rnn.add(Dense(n_classes, activation=activation))\n",
    "    rnn.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return rnn\n",
    "\n",
    "#\n",
    "# Train Model\n",
    "#\n",
    "\n",
    "def train_model(model, model_id, model_dir, X_train, y_train_ohe, X_test, y_test_ohe, epochs=100, batch_size=10, verbosity=1, patience=None):\n",
    "\n",
    "    mc_file_template = '%s/%s-{epoch:03d}.h5' % (model_dir, model_id)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(mc_file_template, monitor='val_loss', \\\n",
    "                                                 verbose=verbosity, save_best_only=True, \\\n",
    "                                                 save_weights_only=False, mode='auto', \\\n",
    "                                                 period=1)\n",
    "    callbacks = [checkpoint]\n",
    "    if patience is not None:\n",
    "        callbacks += [EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "\n",
    "    history = model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test_ohe),\n",
    "               callbacks=callbacks, verbose=verbosity )\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#\n",
    "# hyper parameter tuning\n",
    "#\n",
    "\n",
    "WORD_INDEX_FILENAME_TEMPLATE = '%s/word_index.json'\n",
    "\n",
    "# grid search\n",
    "def tune_hyper_parameters(dataset_name, model_files_dir, max_article_length, categories=None, \\\n",
    "                          test_ratio=0.2, random_seed=42, verbosity=1, epochs=10, batch_size=64):\n",
    "\n",
    "    word_index_file = WORD_INDEX_FILENAME_TEMPLATE % model_files_dir\n",
    "    \n",
    "    #\n",
    "    # Import Dataset and format for testing and training\n",
    "    #\n",
    "    \n",
    "    bunch = load_dataset(DATA_SET_NAME, CATEGORIES)\n",
    "    X_train, X_test, y_train_ohe, y_test_ohe = ready_input(bunch, max_article_length, \\\n",
    "                                                                       word_index_file, \\\n",
    "                                                                       test_ratio=test_ratio, \\\n",
    "                                                                       random_seed=random_seed)\n",
    "\n",
    "    #\n",
    "    # fixed parameters\n",
    "    #\n",
    "    word_index_file = word_index_file\n",
    "    n_classes = len(bunch.target_names) # num_classes\n",
    "    \n",
    "    train_params = {\n",
    "        'model' : None, # dynamically set\n",
    "        'model_id' : None, # dynamically set\n",
    "        'model_dir' : model_files_dir,\n",
    "        'X_train' : X_train,\n",
    "        'X_test' : X_test,\n",
    "        'y_train_ohe' : y_train_ohe,\n",
    "        'y_test_ohe' : y_test_ohe,\n",
    "        'epochs' : epochs,\n",
    "        'batch_size' : batch_size, \n",
    "        'verbosity' : verbosity\n",
    "    }\n",
    "\n",
    "    #\n",
    "    # grid -- tunable params\n",
    "    #\n",
    "    \n",
    "    rnn_types = ['GRU', 'LSTM']\n",
    "    embed_sizes = [50] # [50, 100, 200, 300] # [TODO]: make dynamic\n",
    "    state_sizes = [15*i for i in range(5)]\n",
    "    dropouts = list(np.linspace(0, 0.5, num=2))\n",
    "    recurrent_dropouts = list(np.linspace(0, 0.5, num=2))\n",
    "    dense_activations = ['sigmoid']\n",
    "    optimizers = ['rmsprop']\n",
    "\n",
    "    build_param_ids = ['RNN_type','embed_size' ,'state_size', 'dropout', 'recurrent_dropout', 'activation', 'optimizer']\n",
    "    build_params_set = [rnn_types, embed_sizes, state_sizes, dropouts, recurrent_dropouts, dense_activations, optimizers]\n",
    "\n",
    "    build_param_permutes = itertools.product(*build_params_set)\n",
    "    for i, build_param_values in enumerate(build_param_permutes):\n",
    "        \n",
    "        # set dynamic build parameters\n",
    "        build_params = dict(zip(build_param_ids, build_param_values))\n",
    "        \n",
    "        # add fixed build params\n",
    "        build_params['word_index_file'] = word_index_file\n",
    "        build_params['n_classes'] = n_classes\n",
    "        build_params['max_article_length'] = max_article_length\n",
    "    \n",
    "        # generate model_id \n",
    "        model_id = i\n",
    "        \n",
    "        # save model parameters\n",
    "        save_model_parameters(model_id, build_params, model_files_dir)\n",
    "        \n",
    "        # build\n",
    "        model = build_simple_RNN_model(**build_params)\n",
    "\n",
    "        # display summary\n",
    "        print(model.summary())\n",
    "        \n",
    "        # add model_id and model to train parameters\n",
    "        train_params['model'] = model\n",
    "        train_params['model_id'] = model_id\n",
    "    \n",
    "        # train\n",
    "        history = train_model(**train_params)\n",
    "\n",
    "        # display scores\n",
    "        #plot_training_history(history)\n",
    "        #score_model(X_train, y_train_ohe, model, \"FINAL train\", batch_size)\n",
    "        #score_model(X_test, y_test_ohe, model, \"FINAL test\", batch_size)\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 500, 50)           711250    \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 50)                15150     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 726,655\n",
      "Trainable params: 15,405\n",
      "Non-trainable params: 711,250\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 589 samples, validate on 148 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 1.5531 - acc: 0.3616 - val_loss: 1.5069 - val_acc: 0.3784\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.50686, saving model to data/model_files/0-001.h5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 1.5016 - acc: 0.3939 - val_loss: 1.4789 - val_acc: 0.3851\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.50686 to 1.47890, saving model to data/model_files/0-002.h5\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-667e53e3418b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtune_hyper_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_SET_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_FILES_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_ART_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCATEGORIES\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0mtest_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_2_SET_RATIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVERBOSITY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-6b5530081038>\u001b[0m in \u001b[0;36mtune_hyper_parameters\u001b[0;34m(dataset_name, model_files_dir, max_article_length, categories, test_ratio, random_seed, verbosity, epochs, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# display scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-1fe976a633a0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_id, model_dir, X_train, y_train_ohe, X_test, y_test_ohe, epochs, batch_size, verbosity, patience)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     history = model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test_ohe),\n\u001b[0;32m--> 108\u001b[0;31m                callbacks=callbacks, verbose=verbosity )\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Settings\n",
    "#\n",
    "\n",
    "DATA_SET_NAME = 'bbcsport'\n",
    "CATEGORIES = ['athletics', 'cricket', 'football', 'rugby', 'tennis']\n",
    "MAX_ART_LEN = 500\n",
    "TEST_2_SET_RATIO = 0.2\n",
    "RANDOM_SEED = 42\n",
    "MODEL_FILES_DIR = 'data/model_files'\n",
    "VERBOSITY = 2\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 250\n",
    "\n",
    "# grid search for best parameters\n",
    "tune_hyper_parameters(DATA_SET_NAME, MODEL_FILES_DIR, MAX_ART_LEN, categories=CATEGORIES, \\\n",
    "                     test_ratio=TEST_2_SET_RATIO, random_seed=RANDOM_SEED, verbosity=VERBOSITY, epochs=EPOCHS, batch_size=BATCH_SIZE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
