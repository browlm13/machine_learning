{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import Dataset\n",
    "#\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "DATA_SETS_PATH = 'data/data_sets'\n",
    "DATA_SET_NAME = 'bbcsport'\n",
    "CATEGORIES = ['athletics', 'cricket', 'football', 'rugby', 'tennis']\n",
    "\n",
    "container_path = os.path.join(DATA_SETS_PATH, DATA_SET_NAME)\n",
    "bunch = load_files(container_path=container_path, description=DATA_SET_NAME, categories=CATEGORIES, decode_error='ignore', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_dataset_info_string(dataset):\n",
    "    \"\"\"Ridiculous way to print out dataset info\"\"\"\n",
    "    \n",
    "    # count number of samples/documents in dataset\n",
    "    num_docs = lambda i: list(zip(*np.unique(dataset.target, return_counts=True)))[i][1]\n",
    "    \n",
    "    # ordering of output\n",
    "    display_column_order = ['Target', 'Target Name', 'Documents']\n",
    "    \n",
    "    # uses target as index\n",
    "    column_param_funcs = {\n",
    "        'Target' : lambda i: i,\n",
    "        'Target Name' : lambda i: dataset.target_names[i],\n",
    "        'Documents' : lambda i: num_docs(i)\n",
    "    }\n",
    "    \n",
    "    column_names = list(column_param_funcs.keys())\n",
    "    column_headers_dict = {column_name:column_name for column_name in column_names}\n",
    "    column_values = zip(*[[v(i) for v in column_param_funcs.values()] for i in range(len(dataset.target_names))])\n",
    "\n",
    "    # useful dictionaries \n",
    "    info_dict = [{k:v(i) for k,v in column_param_funcs.items()} for i in range(len(dataset.target_names))]\n",
    "    merged_values_by_column = dict(zip(column_names, column_values))    \n",
    "    \n",
    "    # get maximum length string for each column name in dataset\n",
    "    get_max_str_len = lambda list: max([len(str(i)) for i in list])\n",
    "    max_header_len = {k: max(len(k),get_max_str_len(v)) for k,v in merged_values_by_column.items()}\n",
    "    ordered_max_header_len = [(column_name, max_header_len[column_name]) for column_name in display_column_order] \n",
    "    \n",
    "    # format output\n",
    "    template = '|'.join([\"{%s:%d}\" % (column_name, max_len) for column_name, max_len in ordered_max_header_len])\n",
    "    \n",
    "    # create header\n",
    "    header = template.format(**column_headers_dict)\n",
    "    bar = '-'*(sum([o[1] for o in ordered_max_header_len]) + len(ordered_max_header_len))\n",
    "\n",
    "    # add category info to display string\n",
    "    description = dataset.DESCR\n",
    "    if dataset.DESCR is None:\n",
    "        description = \"None\"\n",
    "    data_set_info_string = 'Dataset Description: \\n' + dataset.DESCR + '\\n' + bar + '\\n' + header + '\\n' + bar + '\\n'\n",
    "    for rec in info_dict: \n",
    "          data_set_info_string += template.format(**rec) + '\\n'\n",
    "    data_set_info_string += bar\n",
    "    \n",
    "    # add total number of documents to string\n",
    "    total_documents = dataset.target.shape[0]\n",
    "    data_set_info_string += \"\\nTotal Documents:\\t\" + str(total_documents)\n",
    "\n",
    "            \n",
    "    return data_set_info_string    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Description: \n",
      "bbcsport\n",
      "-----------------------------\n",
      "Target|Target Name|Documents\n",
      "-----------------------------\n",
      "     0|athletics  |      101\n",
      "     1|cricket    |      124\n",
      "     2|football   |      265\n",
      "     3|rugby      |      147\n",
      "     4|tennis     |      100\n",
      "-----------------------------\n",
      "Total Documents:\t737\n",
      "Vocabulary Size:\t14224\n",
      "\n",
      "Pad and limit inputs to RNN... \n",
      "\n",
      "Maximum Document Length: 1724 (words)\n",
      "Mean Document Length: 346.472184531886 (words)\n",
      "STD Document Lengths: 190.3401998942172 \n",
      "\n",
      "Maximum Document Length Allowed: 500 (words)\n",
      "Chosen Document Length Parameter Effects...\n",
      "\n",
      "Percentage of Documents Within Limit: 82.4966078697422\n",
      "STD's Covered: 0.8065969015133854\n",
      "\n",
      "Encoded Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (737, 500)\n",
      "Shape of Label Tensor: (737, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Description: \n",
      "bbcsport\n",
      "-----------------------------\n",
      "Target|Target Name|Documents\n",
      "-----------------------------\n",
      "     0|athletics  |      101\n",
      "     1|cricket    |      124\n",
      "     2|football   |      265\n",
      "     3|rugby      |      147\n",
      "     4|tennis     |      100\n",
      "-----------------------------\n",
      "Total Documents:\t737\n",
      "Vocabulary Size:\t14224\n",
      "\n",
      "Pad and limit inputs to RNN... \n",
      "\n",
      "Maximum Document Length: 1724 (words)\n",
      "Mean Document Length: 346.472184531886 (words)\n",
      "STD Document Lengths: 190.3401998942172 \n",
      "\n",
      "Maximum Document Length Allowed: 500 (words)\n",
      "Chosen Document Length Parameter Effects...\n",
      "\n",
      "Percentage of Documents Within Limit: 82.4966078697422\n",
      "STD's Covered: 0.8065969015133854\n",
      "\n",
      "Encoded Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (737, 500)\n",
      "Shape of Label Tensor: (737, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "\n",
    "#\n",
    "# Preproses Text, Convert Documents to Sequences, and One-Hot Encode Targets\n",
    "#\n",
    "\n",
    "# tokenize, build vocab and generate document sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(bunch.data)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(bunch.data)\n",
    "word_index = tokenizer.word_index\n",
    "VOCAB_SIZE = len(word_index)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(bunch.target)\n",
    "\n",
    "#\n",
    "# display dataset info\n",
    "#\n",
    "\n",
    "print(get_dataset_info_string(bunch))\n",
    "print(\"Vocabulary Size:\\t\" + str(VOCAB_SIZE))\n",
    "\n",
    "#\n",
    "# padding and clipping documents for reccurent network input\n",
    "#\n",
    "\n",
    "print(\"\\nPad and limit inputs to RNN... \\n\")\n",
    "\n",
    "# pad and clip\n",
    "MAX_ART_LEN = 500\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "# display document lengths and stats\n",
    "document_lengths = np.array([len(d) for d in sequences])\n",
    "MAX_DOCUEMNT_LENGTH, mean_length, std_length = max(document_lengths), np.mean(document_lengths), np.std(document_lengths)\n",
    "\n",
    "print(\"Maximum Document Length: %s (words)\" % MAX_DOCUEMNT_LENGTH)\n",
    "print(\"Mean Document Length: %s (words)\" % mean_length)\n",
    "print(\"STD Document Lengths: %s \" % std_length)\n",
    "sns.distplot(list(document_lengths), kde=False, rug=True);\n",
    "\n",
    "# MAX_ART_LEN parameter effects\n",
    "docs_covered = document_lengths[ np.where( document_lengths <= MAX_ART_LEN ) ]\n",
    "percent_docs_covered = len(docs_covered)/len(document_lengths)\n",
    "stds_covered = (MAX_ART_LEN - mean_length)/std_length\n",
    "\n",
    "print(\"\\nMaximum Document Length Allowed: %s (words)\" % MAX_ART_LEN)\n",
    "print(\"Chosen Document Length Parameter Effects...\\n\")\n",
    "print(\"Percentage of Documents Within Limit: %s\" % (percent_docs_covered*100))\n",
    "print(\"STD's Covered: %s\" % stds_covered)\n",
    "\n",
    "# display final specs\n",
    "print(\"\\nEncoded Dataset Dimensions:\\n\")\n",
    "print('Shape of Data Tensor:', X.shape)\n",
    "print('Shape of Label Tensor:', y_ohe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Description: \n",
      "bbcsport\n",
      "-----------------------------\n",
      "Target|Target Name|Documents\n",
      "-----------------------------\n",
      "     0|athletics  |      101\n",
      "     1|cricket    |      124\n",
      "     2|football   |      265\n",
      "     3|rugby      |      147\n",
      "     4|tennis     |      100\n",
      "-----------------------------\n",
      "Total Documents:\t737\n",
      "\n",
      "Encoded Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (737, 500)\n",
      "Shape of Label Tensor: (737, 5)\n",
      "\n",
      "Train Test Split:  0.2\n",
      "\n",
      "Train Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (589, 500)\n",
      "Shape of Label Tensor: (589, 5)\n",
      "\n",
      "Test Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (148, 500)\n",
      "Shape of Label Tensor: (148, 5)\n",
      "Dataset Description: \n",
      "bbcsport\n",
      "-----------------------------\n",
      "Target|Target Name|Documents\n",
      "-----------------------------\n",
      "     0|athletics  |      101\n",
      "     1|cricket    |      124\n",
      "     2|football   |      265\n",
      "     3|rugby      |      147\n",
      "     4|tennis     |      100\n",
      "-----------------------------\n",
      "Total Documents:\t737\n",
      "\n",
      "Encoded Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (737, 500)\n",
      "Shape of Label Tensor: (737, 5)\n",
      "\n",
      "Train Test Split:  0.2\n",
      "\n",
      "Train Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (589, 500)\n",
      "Shape of Label Tensor: (589, 5)\n",
      "\n",
      "Test Dataset Dimensions:\n",
      "\n",
      "Shape of Data Tensor: (148, 500)\n",
      "Shape of Label Tensor: (148, 5)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Seperate Train and Test Data\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_2_SET_RATIO = 0.2\n",
    "DOUGLAS_ADAMS = 42\n",
    "NUM_CLASSES = len(bunch.target_names)\n",
    "\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=TEST_2_SET_RATIO,\n",
    "                                                            stratify=bunch.target, \n",
    "                                                            random_state=DOUGLAS_ADAMS)\n",
    "#\n",
    "# display dataset train / test split info\n",
    "#\n",
    "\n",
    "print(get_dataset_info_string(bunch))\n",
    "\n",
    "print(\"\\nEncoded Dataset Dimensions:\\n\")\n",
    "print('Shape of Data Tensor:', X.shape)\n",
    "print('Shape of Label Tensor:', y_ohe.shape)\n",
    "\n",
    "print(\"\\nTrain Test Split: \", TEST_2_SET_RATIO)\n",
    "\n",
    "print(\"\\nTrain Dataset Dimensions:\\n\")\n",
    "print('Shape of Data Tensor:', X_train.shape)\n",
    "print('Shape of Label Tensor:', y_train_ohe.shape)\n",
    "\n",
    "print(\"\\nTest Dataset Dimensions:\\n\")\n",
    "print('Shape of Data Tensor:', X_test.shape)\n",
    "print('Shape of Label Tensor:', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Helper Methods\n",
    "#\n",
    "\n",
    "#\n",
    "# create glove embedding matrix from gloves pretrained models\n",
    "#\n",
    "\n",
    "def format_glove_embedding_matrix(dimensions, word_index):\n",
    "    \"\"\" \n",
    "        returns embedding_matrix corresponding to word_index columns\n",
    "        \n",
    "        embdedding_index \n",
    "            format: {key : word, value : word vector}\n",
    "            \n",
    "        Note: unfound words in word_index will be zero vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # the embedding dimensions should match the file you load glove from\n",
    "    assert dimensions in [50, 100, 200, 300]\n",
    "    \n",
    "    GLOVE_EMBEDDINGS_FILE_TEMPLATE = 'data/glove/glove.6B.%sd.txt'\n",
    "    glove_file = GLOVE_EMBEDDINGS_FILE_TEMPLATE % dimensions\n",
    "    \n",
    "    #\n",
    "    # create embeddings index\n",
    "    #\n",
    "    \n",
    "    # format: {key : word, value : word vector} \n",
    "    embeddings_index = {}\n",
    "    \n",
    "    # load glove embeddings file and fill index\n",
    "    f = open(glove_file)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    #\n",
    "    # build embedding matrix coressponding to given word_index\n",
    "    #    Note: words not found in embedding index will be all-zeros.\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, dimensions))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # return index and matrix\n",
    "    return embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#              Grid Search / Hyper Parameter Tuning \n",
    "#\n",
    "#                   selecting model parameters \n",
    "#\n",
    "#\n",
    "\n",
    "#\n",
    "#\n",
    "#           Test/Search Settings -- apply to each model tested\n",
    "#\n",
    "#\n",
    "\n",
    "# - Single Dense Layer Variable Activation\n",
    "# - Shared Glove Embeddings\n",
    "\n",
    "DATA_SETS_PATH = 'data/data_sets'\n",
    "DATA_SET_NAME = 'bbcsport'\n",
    "CATEGORIES = ['athletics', 'cricket', 'football', 'rugby', 'tennis']\n",
    "\n",
    "MAXIMUM_ARTICLE_LENGTH = 500\n",
    "EMBEDDING_DIMENSIONS = 50 # Possible: 50, 100, 200, 300\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "VERBOSE = 2\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "#\n",
    "# Grid Settings\n",
    "#\n",
    "\n",
    "GRID_RNN_TYPE = ['GRU'] #, 'LSTM'] # Possible : 'GRU', 'LSTM'\n",
    "GRID_STATE_SIZE = [10] #[5*i for i in range(20)]\n",
    "GRID_DROPOUT = [0.1] #np.linspace(0, 0.8, 8)\n",
    "GRID_RECURRENT_DROPOUT = [0.1] #np.linspace(0, 0.8, 8)\n",
    "GRID_ACTIVATION = ['sigmoid']\n",
    "GRID_LOSS = ['categorical_crossentropy']\n",
    "GRID_OPTIMIZER = ['rmsprop']\n",
    "GRID_METRICS = [['accuracy']]\n",
    "\n",
    "\n",
    "#\n",
    "# Import Dataset\n",
    "#\n",
    "\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "container_path = os.path.join(DATA_SETS_PATH, DATA_SET_NAME)\n",
    "bunch = load_files(container_path=container_path, description=DATA_SET_NAME, categories=CATEGORIES, decode_error='ignore', encoding='utf-8')\n",
    "\n",
    "#\n",
    "# Build Tokenizer and Embeddings Matrix\n",
    "#   Convert Documents to Sequences, and One-Hot Encode Targets\n",
    "# \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# build tokenizer\n",
    "tokenizer_master = Tokenizer()\n",
    "tokenizer_master.fit_on_texts(bunch.data)\n",
    "\n",
    "# build embeddings matrix\n",
    "embedding_matrix_master = format_glove_embedding_matrix(EMBEDDING_DIMENSIONS, tokenizer_master.word_index)\n",
    "\n",
    "#\n",
    "# Convert Documents to Sequences, and One-Hot Encode Targets\n",
    "#\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenize, build vocab and generate document sequences\n",
    "sequences = tokenizer.texts_to_sequences(bunch.data)\n",
    "\n",
    "# pad and clip\n",
    "X = pad_sequences(sequences, maxlen=MAXIMUM_ARTICLE_LENGTH)\n",
    "y_ohe = keras.utils.to_categorical(bunch.target)\n",
    "\n",
    "#\n",
    "# Split dataset it into train / test subsets\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train / test data\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=TEST_SIZE,\n",
    "                                                            stratify=bunch.target, \n",
    "                                                            random_state=RANDOM_SEED)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class RNNTextClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"RNN Text classifier\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type='GRU', num_outputs=None, state_size=50, dropout=0, recurrent_dropout=0,\n",
    "            activation='sigmoid', loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "               metrics=['accuracy'], max_input_len=500, embeddings_matrix=None, tokenizer=None):\n",
    "  \n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            #print(\"{} = {}\".format(arg,val))\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=100, batch_size=1, verbose=2):\n",
    "\n",
    "        # build model\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # build embedding layer\n",
    "        self.embedding_size = self.embeddings_matrix.shape[1]\n",
    "        embedding_layer = Embedding(len(self.tokenizer.word_index) + 1, self.embedding_size,\n",
    "                            weights=[self.embeddings_matrix], input_length=self.max_input_len,\n",
    "                            trainable=False)\n",
    "        self.model.add(embedding_layer)\n",
    "        \n",
    "        # build recurent layer\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            self.model.add(LSTM(self.embedding_size, dropout=self.dropout, recurrent_dropout=self.recurrent_dropout ))\n",
    "        else:\n",
    "            self.model.add(GRU(self.embedding_size, dropout=self.dropout, recurrent_dropout=self.recurrent_dropout ))\n",
    "        \n",
    "        # build single dense layer\n",
    "        self.model.add(Dense(self.num_outputs, activation=self.activation))\n",
    "        \n",
    "        # compile model\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "        \n",
    "        # fit model\n",
    "        self.model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=batch_size, validation_data=validation_data, verbose=verbose )\n",
    "               #callbacks=callbacks, verbose=verbose )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y, batch_size=None, verbose=1):\n",
    "        return self.model.evaluate(X, y,batch_size=batch_size, verbose=verbose)\n",
    "        \n",
    "    def summary(self):\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def get_params(self,deep=False):\n",
    "        # fix this\n",
    "        return vars(self)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Simple RNN class for building and saving models\n",
    "#\n",
    "\n",
    "#\n",
    "# tried making a custom estimator but ran into issues.\n",
    "# this is used for easy saving and loading of models\n",
    "#\n",
    "\n",
    "import inspect\n",
    "#import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Embedding\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "class text_RNN:\n",
    "    \n",
    "    def __init__(self, rnn_type='GRU', num_outputs=None, \\\n",
    "                 state_size=50, dropout=0, recurrent_dropout=0, \\\n",
    "                 activation='sigmoid', loss='categorical_crossentropy', \\\n",
    "                 optimizer='rmsprop', metrics=['accuracy']):\n",
    "\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            #print(\"{} = {}\".format(arg,val))\n",
    "            setattr(self, arg, val)\n",
    "            \n",
    "        # enforce a few settings\n",
    "        assert self.rnn_type in ['LSTM', 'GRU']\n",
    "        \n",
    "    def compile_model(self, num_outputs=None, max_input_len=None, embeddings_matrix=None, tokenizer=None):\n",
    "        \n",
    "        assert num_outputs is not None\n",
    "        assert max_input_len is not None\n",
    "        assert embeddings_matrix is not None\n",
    "        assert tokenizer is not None\n",
    "        \n",
    "        self.num_outputs = num_outputs\n",
    "        self.max_input_len = max_input_len\n",
    "        self.embeddings_matrix = embeddings_matrix\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # build model\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # build embedding layer\n",
    "        self.embedding_size = self.embeddings_matrix.shape[1]\n",
    "        embedding_layer = Embedding(len(self.tokenizer.word_index) + 1, self.embedding_size,\n",
    "                            weights=[self.embeddings_matrix], input_length=self.max_input_len,\n",
    "                            trainable=False)\n",
    "        self.model.add(embedding_layer)\n",
    "        \n",
    "        # build recurent layer\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            self.model.add(LSTM(self.embedding_size, dropout=self.dropout, recurrent_dropout=self.recurrent_dropout ))\n",
    "        else:\n",
    "            self.model.add(GRU(self.embedding_size, dropout=self.dropout, recurrent_dropout=self.recurrent_dropout ))\n",
    "        \n",
    "        # build single dense layer\n",
    "        self.model.add(Dense(self.num_outputs, activation=self.activation))\n",
    "        \n",
    "        # compile model\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "\n",
    "    # ah the worst\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "#\n",
    "# Ugly wrapper method for KerasClassifier, GridSearchCV and cross_val_score\n",
    "# tried making a custom estimator but ran into issues \n",
    "#\n",
    "\n",
    "#\n",
    "# build, compile model and return model\n",
    "#\n",
    "\n",
    "def create_model(rnn_type=None, num_outputs=None, \\\n",
    "                 state_size=None, dropout=None, recurrent_dropout=None, \\\n",
    "                 activation=None, loss=None, \\\n",
    "                 optimizer=None, metrics=None, \\\n",
    "                 max_input_len=None, embeddings_matrix=None, tokenizer=None):\n",
    "\n",
    "    rnn_co = text_RNN(rnn_type=rnn_type, num_outputs=num_outputs, \\\n",
    "                 state_size=state_size, dropout=dropout, recurrent_dropout=dropout, \\\n",
    "                 activation=activation, loss=loss, \\\n",
    "                 optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    rnn_co.compile_model(num_outputs=len(bunch.target_names), max_input_len=max_input_len, embeddings_matrix=embeddings_matrix, tokenizer=tokenizer)\n",
    "\n",
    "    return rnn_co.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.6442 - acc: 0.1658\n",
      " - 1s - loss: 1.6442 - acc: 0.1658\n",
      "Epoch 1/1\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.6278 - acc: 0.1908\n",
      " - 1s - loss: 1.6278 - acc: 0.1908\n",
      "Epoch 1/1\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.6208 - acc: 0.2061\n",
      " - 1s - loss: 1.6208 - acc: 0.2061\n",
      "Epoch 1/1\n",
      "Epoch 1/1\n",
      " - 2s - loss: 1.6479 - acc: 0.1749\n",
      " - 2s - loss: 1.6479 - acc: 0.1749\n",
      "Best: 0.293718 using {'metrics': ['accuracy'], 'rnn_type': 'GRU', 'state_size': 10, 'embeddings_matrix': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
      "        -0.11514   , -0.78580999],\n",
      "       [ 0.68046999, -0.039263  ,  0.30186   , ..., -0.073297  ,\n",
      "        -0.064699  , -0.26043999],\n",
      "       ...,\n",
      "       [-0.024146  , -0.1061    , -0.43426999, ...,  0.78873998,\n",
      "        -0.33500001,  1.18540001],\n",
      "       [-0.24098   , -0.39517   ,  0.0020529 , ...,  0.79488999,\n",
      "         0.21684   ,  0.23828   ],\n",
      "       [-0.16064   , -1.11240005, -0.21125001, ...,  0.13829   ,\n",
      "         0.78460002,  1.31529999]]), 'recurrent_dropout': 0.1, 'max_input_len': 500, 'activation': 'sigmoid', 'tokenizer': <keras_preprocessing.text.Tokenizer object at 0x1a28b93198>, 'optimizer': 'rmsprop', 'loss': 'categorical_crossentropy', 'dropout': 0.1, 'num_outputs': 5}\n",
      "0.293718 (0.010417) with: {'metrics': ['accuracy'], 'rnn_type': 'GRU', 'state_size': 10, 'embeddings_matrix': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
      "        -0.11514   , -0.78580999],\n",
      "       [ 0.68046999, -0.039263  ,  0.30186   , ..., -0.073297  ,\n",
      "        -0.064699  , -0.26043999],\n",
      "       ...,\n",
      "       [-0.024146  , -0.1061    , -0.43426999, ...,  0.78873998,\n",
      "        -0.33500001,  1.18540001],\n",
      "       [-0.24098   , -0.39517   ,  0.0020529 , ...,  0.79488999,\n",
      "         0.21684   ,  0.23828   ],\n",
      "       [-0.16064   , -1.11240005, -0.21125001, ...,  0.13829   ,\n",
      "         0.78460002,  1.31529999]]), 'recurrent_dropout': 0.1, 'max_input_len': 500, 'activation': 'sigmoid', 'tokenizer': <keras_preprocessing.text.Tokenizer object at 0x1a28b93198>, 'optimizer': 'rmsprop', 'loss': 'categorical_crossentropy', 'dropout': 0.1, 'num_outputs': 5}\n",
      "Best: 0.293718 using {'metrics': ['accuracy'], 'rnn_type': 'GRU', 'state_size': 10, 'embeddings_matrix': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
      "        -0.11514   , -0.78580999],\n",
      "       [ 0.68046999, -0.039263  ,  0.30186   , ..., -0.073297  ,\n",
      "        -0.064699  , -0.26043999],\n",
      "       ...,\n",
      "       [-0.024146  , -0.1061    , -0.43426999, ...,  0.78873998,\n",
      "        -0.33500001,  1.18540001],\n",
      "       [-0.24098   , -0.39517   ,  0.0020529 , ...,  0.79488999,\n",
      "         0.21684   ,  0.23828   ],\n",
      "       [-0.16064   , -1.11240005, -0.21125001, ...,  0.13829   ,\n",
      "         0.78460002,  1.31529999]]), 'recurrent_dropout': 0.1, 'max_input_len': 500, 'activation': 'sigmoid', 'tokenizer': <keras_preprocessing.text.Tokenizer object at 0x1a28b93198>, 'optimizer': 'rmsprop', 'loss': 'categorical_crossentropy', 'dropout': 0.1, 'num_outputs': 5}\n",
      "0.293718 (0.010417) with: {'metrics': ['accuracy'], 'rnn_type': 'GRU', 'state_size': 10, 'embeddings_matrix': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
      "        -0.11514   , -0.78580999],\n",
      "       [ 0.68046999, -0.039263  ,  0.30186   , ..., -0.073297  ,\n",
      "        -0.064699  , -0.26043999],\n",
      "       ...,\n",
      "       [-0.024146  , -0.1061    , -0.43426999, ...,  0.78873998,\n",
      "        -0.33500001,  1.18540001],\n",
      "       [-0.24098   , -0.39517   ,  0.0020529 , ...,  0.79488999,\n",
      "         0.21684   ,  0.23828   ],\n",
      "       [-0.16064   , -1.11240005, -0.21125001, ...,  0.13829   ,\n",
      "         0.78460002,  1.31529999]]), 'recurrent_dropout': 0.1, 'max_input_len': 500, 'activation': 'sigmoid', 'tokenizer': <keras_preprocessing.text.Tokenizer object at 0x1a28b93198>, 'optimizer': 'rmsprop', 'loss': 'categorical_crossentropy', 'dropout': 0.1, 'num_outputs': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#results = cross_val_score(model, X_train,y_train_ohe)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n#results = cross_val_score(model, X_train,y_train_ohe)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Grid Search / Hyper Parameter Tuning and Cross Validation\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "#from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#\n",
    "# Create Estimator\n",
    "#\n",
    "\n",
    "# Keras Classifier Wrapper\n",
    "model = KerasClassifier(build_fn=create_model,\n",
    "                        epochs=EPOCHS, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        verbose=VERBOSE )\n",
    "#\n",
    "# Setup Grid\n",
    "#\n",
    "\n",
    "param_grid = {\n",
    "    \n",
    "    # const\n",
    "    'max_input_len' : [MAXIMUM_ARTICLE_LENGTH],\n",
    "    'num_outputs' : [len(bunch.target_names)],\n",
    "    'embeddings_matrix': [embedding_matrix_master],\n",
    "    'tokenizer' : [tokenizer_master],\n",
    "    \n",
    "    # variable\n",
    "    'rnn_type' : GRID_RNN_TYPE,\n",
    "    'state_size' : GRID_STATE_SIZE, \n",
    "    'dropout' : GRID_DROPOUT, \n",
    "    'recurrent_dropout' : GRID_RECURRENT_DROPOUT,\n",
    "    'activation' : GRID_ACTIVATION,\n",
    "    'loss' : GRID_LOSS,\n",
    "    'optimizer' : GRID_OPTIMIZER,\n",
    "    'metrics' : GRID_METRICS\n",
    "}\n",
    "\n",
    "#\n",
    "# Perform Grid Search\n",
    "#\n",
    "\n",
    "grid = GridSearchCV(model,param_grid=param_grid, refit='precision_macro') #return_train_score=True,\n",
    "grid_results = grid.fit(X_train,y_train_ohe) \n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_results.best_score_, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/keras/engine/sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-40cc5e5368b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-40cc5e5368b6>\u001b[0m in \u001b[0;36mplot_training_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(history.history.keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/keras/engine/sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-40cc5e5368b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-40cc5e5368b6>\u001b[0m in \u001b[0;36mplot_training_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(history.history.keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_training_history(history):\n",
    "    # [TODO]: make not shitty\n",
    "    #print(history.history.keys())\n",
    "\n",
    "    pyplot.plot(history.history['acc'], label='accuracy')\n",
    "    pyplot.plot(history.history['loss'], label='categorical_crossentropy')\n",
    "\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "history = grid_results.best_estimator_.model.model.history.history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_results.best_score_, grid_results.best_params_))\n",
    "\n",
    "\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    \n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_results.best_score_, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,\n",
    "\"\"\"\n",
    "#results = cross_val_score(model, X_train,y_train_ohe)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
